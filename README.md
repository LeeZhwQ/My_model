# My_model
transformer 


training steps: 10000



about 0.2B parameters


具体调用："洗漱"


实际训练维度为512，注意力头为8，context_length为32，代码中有所上调


<img width="1300" height="125" alt="image" src="https://github.com/user-attachments/assets/65a3a944-03a7-4087-94fa-0243aa2bab9a" />
